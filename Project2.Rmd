---
title: "Principal Component Regression"
author: "Grant Bailey, Lizhou Wang, Andrew Li"
date: "12/3/2020"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, # Required
                      fig.path = "./figures/",
                      fig.width = 10,
                      fig.height = 5,
                      message = FALSE, # Turn off load messages
                      warning = FALSE # Turn off warnings
                      )
knitr::opts_knit$set(root.dir = 'data')
library(scales)
library(dplyr)
library(ggplot2)
library(GGally)
library(corrplot)
library(MASS)
library(clusterSim)
library(pls)
```

# Mathematical Theory
## Introduction

\small

Let's look at some random data. Here we linearly regress $y$ against $x$. I've added red lines to show the two directions we have our normally distributed random data going.
\tiny
```{r, echo=TRUE}
set.seed(0)
x <- rnorm(1000, 0, 3)
y <- rnorm(1000, 0, 1)
mod <- lm(y ~ x)
```

```{r, fig.width=10, fig.height=5}
A <- cbind(x,y)
plot(
  A[, 2] ~ A[, 1],
  xlim = c(-10, 10),
  ylim = c(-10, 10),
  pch = 16,
  col = alpha("black", .5),
  xlab = "x",
  ylab = "y"
)
mod <- lm(A[,2] ~ A[,1])
lines(mod$fitted.values ~ A[,1], col="blue", lwd=3)
segments(-7, 0, 7, 0, col="red", lwd=3)
segments(0, 3, 0, -3, col="red", lwd=3)
```

```{r, echo=FALSE}
summary(mod)$coefficients
```

## Rotation
\small

Now let's rotate the data and the lines, and fit another regression model with our new dimensions.
\tiny
\begin{align*}
  A &= 
    \begin{bmatrix}
    \vec{x} & \vec{y} \\
    \end{bmatrix} \\
  \theta &= \frac{-\pi}{3} \\
  R &=
    \begin{bmatrix}
    cos(\theta) & -sin(\theta) \\
    sin(\theta) & cos(\theta) 
    \end{bmatrix} \\
  B &= AR
\end{align*}

```{r, fig.width=10, fig.height=5}
A <- cbind(x,y)
th <- -pi/3
R <- matrix(c(cos(th), sin(th), -sin(th), cos(th)), ncol=2)
B <- A %*% R
plot(B[,2] ~ B[,1], xlim = c(-10, 10), ylim=c(-10,10), pch=16, col=alpha("black", .5))
mod <- lm(B[,2] ~ B[,1])
lines(mod$fitted.values ~ B[,1], col="blue", lwd=3, xlab="x", ylab="y")

c <- matrix(c(-7, 0, 7, 0), byrow = T, ncol=2) %*% R
d <- matrix(c(0, -3, 0, 3), byrow = T, ncol=2) %*% R
segments(c[1,1], c[1,2], c[2,1], c[2,2], col="red", lwd=3)
segments(d[1,1], d[1,2], d[2,1], d[2,2], col="red", lwd=3)
```

## Principal Components
\small
Principal components are directions that maximize variance in a matrix. They can also be seen as "axes" for the matrix, which is apparent in the previous plot. Principal components are, then, the same as eigenvectors. Here are the eigenvectors of our matrix $B$ scaled and drawn on our plot.

\tiny

```{r, fig.height=6}
e <- eigen(t(B) %*% B)
v <- e$vectors
plot(B[,2] ~ B[,1], xlim = c(-10, 10), ylim=c(-10,10), pch=16, col=alpha("black", .5))
segments(c[1,1], c[1,2], c[2,1], c[2,2], col="red", lwd=3)
segments(d[1,1], d[1,2], d[2,1], d[2,2], col="red", lwd=3)
arrows(0, 0, x1 = v[1,1]*6, y1 = v[2,1]*6, length = .2, col="deepskyblue", lwd=5)
arrows(0, 0, x1 = v[1,2]*2, y1 = v[2,2]*2, length = .2, col="deepskyblue", lwd=5)
```

## Mathematical Steps
\small
Linear regression is denoted by the formula $y = X \beta + \epsilon$.

Here are the steps to use our principal components for regression:

1. Normalize both our $X$ and our $y$
2. Obtain singular values and eigenvectors for $X$, where $D$ denotes a diagonal matrix with eigenvalues ordered from greatest to least, and $V$ denotes a matrix with eigenvectors for columns.
3. Reduce the number of columns in $V$ based on the eigenvalues.
4. Let $n$ be the total number of columns, $p$ be the initial number of eigenvalues, and $r$ be the chosen number of eigenvectors to retain. Calculate $Z_{n \times r} = X_{n \times p}V_{p \times r}$
5. Calculate $\beta_Z = (Z^\intercal Z)^{-1} Z^\intercal y$
6. Calculate $\hat{y} = Z \beta_{Z} = X \beta_{X}$, where $\beta_X=V\beta_Z$


## Dimension reduction
\small
One major benefit of principal component regression is its ability to reduce the dimensions of our predictor matrix while retaining as much information as possible, which aids in reducing collinearity. Here are the eigenvalues of a randomly generated $10 \times 10$ matrix.

```{r}
n <- 10
set.seed(0)
A <- rnorm(n)
for (i in (2:n)) {
  A <- cbind(A, rnorm(n))
}
A <- data.Normalization(A, "n12") # Centers and normalizes columns of A
A.eig <- eigen(t(A) %*% A)
D <- A.eig$values
V <- A.eig$vectors
plot(D, xaxt = "n", pch=16, cex=1.5, type="b", main = "Eigenvalues of A", ylab = expression(lambda))
axis(side=1, at=c(1:n))
```

## Dimension Reduction
\footnotesize
These eigenvalues reveal how much each eigenvector contributes to the total variance of the matrix. If $\vec{\lambda}$ is the vector of eigenvalues, then $\frac{100}{\sum{\vec{\lambda}}} \vec{\lambda}$ tells us what percentage of the variance is explained by each eigenvalue.
```{r}
D.perc <- D / sum(D) * 100
plot(
  D.perc,
  xaxt = "n",
  pch = 16,
  cex = 1.5,
  type = "b",
  main = " ",
  ylab = "%"
)
axis(side=1, at=c(1:n))
```

The first eigenvector captures nearly 33\% of variance, while the tenth eigenvector contributes nearly nothing, implying that it is nearly a linear combination of other vectors. There is a steep drop-off after our sixth eigenvalue, so six eigenvectors might be a good choice for our regression.

## Dimension Reduction
\footnotesize
If we cumulatively sum this vector of percentage contributions. using only the first five eigenvectors, we capture 89\% of variance. Using the first six eigenvectors, we capture 96\% of the variance. After that, the contributions taper off.

```{r, fig.height=6}
plot(
  cumsum(D.perc),
  pch = 16,
  cex = 1.5,
  type = "b",
  main = "Cumulative Percent of Variance Explained by Eigenvalues",
  ylab = "Cumulative %",
  xaxt = "n",
  yaxt = "n"
)
axis(side=1, at=c(1:n))
axis(side=2, at=round(cumsum(D.perc))[c(1:6)], lwd.ticks=.5)
```

# Examples

## IMDb Data
\small
Let's now use principal component analysis 
```{r}
data <- read.csv("IMDb movies.csv")
head(data)
```


## Next slide
\small
Extract columns of movie duration, average vote, metascore for each movie, reviews from users and reviews from critics and combine them to a new dataframe

```{r pressure}
my_data<-data.frame(data$duration,data$avg_vote,data$metascore,data$reviews_from_users,data$reviews_from_critics)
my_data<-na.omit(my_data)
names(my_data)[1]<-"duration"
names(my_data)[2]<-"avg_vote"
names(my_data)[3]<-"metascore"
names(my_data)[4]<-"reviews_from_users"
names(my_data)[5]<-"reviews_from_critics"
dim(my_data)
```
We will consider metascore as dependent variable




normalized data before we get to PCR 
```{r}
my_data<-data.Normalization(my_data, type="n1", normalization = "column")
```

compute PCA for my_data
```{r}
my_data.pca<-prcomp(my_data, center = TRUE, scale. = TRUE)

## principle component

head(my_data.pca$x)
```
extract principle component from our pca output and use them to do principle component regression

```{r}
metascore<-my_data$metascore
pc<-data.frame(my_data.pca$x)
pcr_dataset<-cbind(metascore,pc)
head(pcr_dataset)
mod<-lm(metascore~., data = pcr_dataset)
summary(mod)
```
return estimate of coefficients from mod output and calculate beta x
```{r}
betaZ<-as.matrix(mod$coeff[2:6])
v<-as.matrix(my_data.pca$rotation)
betaX<-v%*%betaZ
betaX
```


Also, in R there is a package called pls that provide function pcr that can directly give us the model we want. However, we still need to normalize our data in the first place



divide the dataset into test set and training set
```{r}
smp_size <- floor(0.80 * nrow(my_data))

## set the seed 
set.seed(123)
train_ind <- sample(seq_len(nrow(my_data)), size = smp_size)

train <- my_data[train_ind, ]
test <- my_data[-train_ind, ]
dim(test)
dim(train)
```

```{r}
pcr_model<-pcr(metascore~., data = train, scale = TRUE, validation = "CV")
summary(pcr_model)
```


## compare to MLR

\small

I will use a small example to show how PCR is better than MLR in some cases
```{r}
summary(mtcars)
```
\small
split dataset into training set and test set
```{r}
smp_size <- floor(0.80 * nrow(mtcars))

## set the seed 
set.seed(123)
train_ind <- sample(seq_len(nrow(mtcars)), size = smp_size)

train_set <- mtcars[train_ind, ]
test_set <- mtcars[-train_ind, ]

```
\small
This time let's analyze how mpg can be affected by all the other factors.


let's try multiple linear regression first 
```{r}
mlr<-lm(mpg~., data = train_set)
summary(mlr)
```
 
```{r}
mlr_pred<-predict(mlr,test_set)
mean((mlr_pred-test_set$mpg)^2)
```
\small

now let's try to do pcr for this dataset
```{r}
pcr<-pcr(mpg~., data = train_set, scale = TRUE, validation = "CV")
summary(pcr)
```
```{r}
pcr_pred<-predict(pcr, test_set, ncomp = 7)
mean((pcr_pred-test_set$mpg)^2)
```

From the above we can see that our mse is less for PCR than for MLR


