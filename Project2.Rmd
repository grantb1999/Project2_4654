---
title: "Principal Component Regression"
author: "Grant Bailey, Lizhou Wang, Andrew Li"
date: "12/3/2020"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, # Required
                      fig.path = "./figures/",
                      fig.width = 10,
                      fig.height = 5,
                      message = FALSE, # Turn off load messages
                      warning = FALSE # Turn off warnings
                      )
library(scales)
library(dplyr)
library(ggplot2)
library(GGally)
library(corrplot)
library(MASS)
library(clusterSim)
```

# Mathematical Theory
## Introduction

\small

Let's look at some random data. Here we linearly regress $y$ against $x$. I've added red lines to show the two directions we have our normally distributed random data going.
\tiny
```{r, echo=TRUE}
set.seed(0)
x <- rnorm(1000, 0, 3)
y <- rnorm(1000, 0, 1)
mod <- lm(y ~ x)
```

```{r, fig.width=10, fig.height=5}
A <- cbind(x,y)
plot(
  A[, 2] ~ A[, 1],
  xlim = c(-10, 10),
  ylim = c(-10, 10),
  pch = 16,
  col = alpha("black", .5),
  xlab = "x",
  ylab = "y"
)
mod <- lm(A[,2] ~ A[,1])
lines(mod$fitted.values ~ A[,1], col="blue", lwd=3)
segments(-7, 0, 7, 0, col="red", lwd=3)
segments(0, 3, 0, -3, col="red", lwd=3)
```

```{r, echo=FALSE}
summary(mod)$coefficients
```

## Rotation
\small

Now let's rotate the data and the lines, and fit another regression model with our new dimensions.
\tiny
\begin{align*}
  A &= 
    \begin{bmatrix}
    \vec{x} & \vec{y} \\
    \end{bmatrix} \\
  \theta &= \frac{-\pi}{3} \\
  R &=
    \begin{bmatrix}
    cos(\theta) & -sin(\theta) \\
    sin(\theta) & cos(\theta) 
    \end{bmatrix} \\
  B &= AR
\end{align*}

```{r, fig.width=10, fig.height=5}
A <- cbind(x,y)
th <- -pi/3
R <- matrix(c(cos(th), sin(th), -sin(th), cos(th)), ncol=2)
B <- A %*% R
plot(B[,2] ~ B[,1], xlim = c(-10, 10), ylim=c(-10,10), pch=16, col=alpha("black", .5))
mod <- lm(B[,2] ~ B[,1])
lines(mod$fitted.values ~ B[,1], col="blue", lwd=3, xlab="x", ylab="y")

c <- matrix(c(-7, 0, 7, 0), byrow = T, ncol=2) %*% R
d <- matrix(c(0, -3, 0, 3), byrow = T, ncol=2) %*% R
segments(c[1,1], c[1,2], c[2,1], c[2,2], col="red", lwd=3)
segments(d[1,1], d[1,2], d[2,1], d[2,2], col="red", lwd=3)
```

## Principal Components
\small
Principal components are directions that maximize variance in a matrix. They can also be seen as "axes" for the matrix, which is apparent in the previous plot. Principal components are, then, the same as eigenvectors. Here are the eigenvectors of our matrix $B$ scaled and drawn on our plot.

\tiny

```{r, fig.height=6}
e <- eigen(t(B) %*% B)
v <- e$vectors
plot(B[,2] ~ B[,1], xlim = c(-10, 10), ylim=c(-10,10), pch=16, col=alpha("black", .5))
segments(c[1,1], c[1,2], c[2,1], c[2,2], col="red", lwd=3)
segments(d[1,1], d[1,2], d[2,1], d[2,2], col="red", lwd=3)
arrows(0, 0, x1 = v[1,1]*6, y1 = v[2,1]*6, length = .2, col="deepskyblue", lwd=5)
arrows(0, 0, x1 = v[1,2]*2, y1 = v[2,2]*2, length = .2, col="deepskyblue", lwd=5)
```

## Mathematical Steps
\small
Linear regression is denoted by the formula $y = X \beta + \epsilon$.

Here are the steps to use our principal components for regression:

1. Normalize both our $X$ and our $y$
2. Obtain singular values and eigenvectors for $X$, where $D$ denotes a diagonal matrix with eigenvalues ordered from greatest to least, and $V$ denotes a matrix with eigenvectors for columns.
3. Reduce the number of columns in $V$ based on the eigenvalues.
4. Let $n$ be the total number of columns, $p$ be the initial number of eigenvalues, and $r$ be the chosen number of eigenvectors to retain. Calculate $Z_{n \times r} = X_{n \times p}V_{p \times r}$
5. Calculate $\beta_Z = (Z^\intercal Z)^{-1} Z^\intercal y$
6. Calculate $\hat{y} = Z \beta_{Z} = X \beta_{X}$, where $\beta_X=V\beta_Z$


## Dimension reduction
\small
One major benefit of principal component regression is its ability to reduce the dimensions of our predictor matrix while retaining as much information as possible, which aids in reducing collinearity. Here are the eigenvalues of a randomly generated $10 \times 10$ matrix.

```{r}
n <- 10
set.seed(0)
A <- rnorm(n)
for (i in (2:n)) {
  A <- cbind(A, rnorm(n))
}
A <- data.Normalization(A, "n12") # Centers and normalizes columns of A
A.eig <- eigen(t(A) %*% A)
D <- A.eig$values
V <- A.eig$vectors
plot(D, xaxt = "n", pch=16, cex=1.5, type="b", main = "Eigenvalues of A", ylab = expression(lambda))
axis(side=1, at=c(1:n))
```

## Dimension Reduction
\footnotesize
These eigenvalues reveal how much each eigenvector contributes to the total variance of the matrix. If $\vec{\lambda}$ is the vector of eigenvalues, then $\frac{100}{\sum{\vec{\lambda}}} \vec{\lambda}$ tells us what percentage of the variance is explained by each eigenvalue.
```{r}
D.perc <- D / sum(D) * 100
plot(
  D.perc,
  xaxt = "n",
  pch = 16,
  cex = 1.5,
  type = "b",
  main = " ",
  ylab = "%"
)
axis(side=1, at=c(1:n))
```

The first eigenvector captures nearly 33\% of variance, while the tenth eigenvector contributes nearly nothing, implying that it is nearly a linear combination of other vectors. There is a steep drop-off after our sixth eigenvalue, so six eigenvectors might be a good choice for our regression.

## Dimension Reduction
\footnotesize
If we cumulatively sum this vector of percentage contributions. using only the first five eigenvectors, we capture 89\% of variance. Using the first six eigenvectors, we capture 96\% of the variance. After that, the contributions taper off.

```{r, fig.height=6}
plot(
  cumsum(D.perc),
  pch = 16,
  cex = 1.5,
  type = "b",
  main = "Cumulative Percent of Variance Explained by Eigenvalues",
  ylab = "Cumulative %",
  xaxt = "n",
  yaxt = "n"
)
axis(side=1, at=c(1:n))
axis(side=2, at=round(cumsum(D.perc))[c(1:6)], lwd.ticks=.5)
```

# Examples




