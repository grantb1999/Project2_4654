---
title: "Principal Component Regression"
author: "Grant Bailey, Lizhou Wang, Andrew Li"
date: "12/3/2020"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, # Required
                      fig.path = "./figures/",
                      fig.width = 10,
                      fig.height = 5,
                      message = FALSE, # Turn off load messages
                      warning = FALSE # Turn off warnings
                      )
knitr::opts_knit$set(root.dir = 'data')
library(scales)
library(dplyr)
library(ggplot2)
library(GGally)
library(corrplot)
library(MASS)
library(clusterSim)
library(pls)
library(knitr)
library(car)
```

# Mathematical Theory
## Introduction

\small

Let's look at some random data. Here we linearly regress $y$ against $x$. I've added red lines to show the two directions we have our normally distributed random data going.
\tiny
```{r, echo=TRUE}
set.seed(0)
x <- rnorm(1000, 0, 3)
y <- rnorm(1000, 0, 1)
mod <- lm(y ~ x)
```

```{r, fig.width=10, fig.height=5}
A <- cbind(x,y)
plot(
  A[, 2] ~ A[, 1],
  xlim = c(-10, 10),
  ylim = c(-10, 10),
  pch = 16,
  col = alpha("black", .5),
  xlab = "x",
  ylab = "y"
)
mod <- lm(A[,2] ~ A[,1])
lines(mod$fitted.values ~ A[,1], col="blue", lwd=3)
segments(-7, 0, 7, 0, col="red", lwd=3)
segments(0, 3, 0, -3, col="red", lwd=3)
```

```{r, echo=FALSE}
summary(mod)$coefficients
```

## Rotation
\small

Now let's rotate the data and the lines, and fit another regression model with our new dimensions.
\tiny
\begin{align*}
  A &= 
    \begin{bmatrix}
    \vec{x} & \vec{y} \\
    \end{bmatrix} \\
  \theta &= \frac{-\pi}{3} \\
  R &=
    \begin{bmatrix}
    cos(\theta) & -sin(\theta) \\
    sin(\theta) & cos(\theta) 
    \end{bmatrix} \\
  B &= AR
\end{align*}

```{r, fig.width=10, fig.height=5}
A <- cbind(x,y)
th <- -pi/3
R <- matrix(c(cos(th), sin(th), -sin(th), cos(th)), ncol=2)
B <- A %*% R
plot(B[,2] ~ B[,1], xlim = c(-10, 10), ylim=c(-10,10), pch=16, col=alpha("black", .5))
mod <- lm(B[,2] ~ B[,1])
lines(mod$fitted.values ~ B[,1], col="blue", lwd=3, xlab="x", ylab="y")

c <- matrix(c(-7, 0, 7, 0), byrow = T, ncol=2) %*% R
d <- matrix(c(0, -3, 0, 3), byrow = T, ncol=2) %*% R
segments(c[1,1], c[1,2], c[2,1], c[2,2], col="red", lwd=3)
segments(d[1,1], d[1,2], d[2,1], d[2,2], col="red", lwd=3)
```

## Principal Components
\small
Principal components are directions that maximize variance in a matrix. They can also be seen as "axes" for the matrix, which is apparent in the previous plot. Principal components are, then, the same as eigenvectors. Here are the eigenvectors of our matrix $B$ scaled and drawn on our plot.

\tiny

```{r, fig.height=6}
e <- eigen(t(B) %*% B)
v <- e$vectors
plot(B[,2] ~ B[,1], xlim = c(-10, 10), ylim=c(-10,10), pch=16, col=alpha("black", .5))
segments(c[1,1], c[1,2], c[2,1], c[2,2], col="red", lwd=3)
segments(d[1,1], d[1,2], d[2,1], d[2,2], col="red", lwd=3)
arrows(0, 0, x1 = v[1,1]*6, y1 = v[2,1]*6, length = .2, col="deepskyblue", lwd=5)
arrows(0, 0, x1 = v[1,2]*2, y1 = v[2,2]*2, length = .2, col="deepskyblue", lwd=5)
```

## Mathematical Steps
\small
Linear regression is denoted by the formula $y = X \beta + \epsilon$.

Here are the steps to use our principal components for regression:

1. Normalize both our $X$ and our $y$
2. Obtain singular values and eigenvectors for $X$, where $D$ denotes a diagonal matrix with eigenvalues ordered from greatest to least, and $V$ denotes a matrix with eigenvectors for columns.
3. Reduce the number of columns in $V$ based on the eigenvalues.
4. Let $n$ be the total number of columns, $p$ be the initial number of eigenvalues, and $r$ be the chosen number of eigenvectors to retain. Calculate $Z_{n \times r} = X_{n \times p}V_{p \times r}$
5. Calculate $\beta_Z = (Z^\intercal Z)^{-1} Z^\intercal y$
6. Calculate $\hat{y} = Z \beta_{Z} = X \beta_{X}$, where $\beta_X=V\beta_Z$


## Dimension reduction
\small
One major benefit of principal component regression is its ability to reduce the dimensions of our predictor matrix while retaining as much information as possible, which aids in reducing collinearity. Here are the eigenvalues of a randomly generated $10 \times 10$ matrix.

```{r}
n <- 10
set.seed(0)
A <- rnorm(n)
for (i in (2:n)) {
  A <- cbind(A, rnorm(n))
}
A <- data.Normalization(A, "n12") # Centers and normalizes columns of A
A.eig <- eigen(t(A) %*% A)
D <- A.eig$values
V <- A.eig$vectors
plot(D, xaxt = "n", pch=16, cex=1.5, type="b", main = "Eigenvalues of A", ylab = expression(lambda))
axis(side=1, at=c(1:n))
```

## Dimension Reduction
\footnotesize
These eigenvalues reveal how much each eigenvector contributes to the total variance of the matrix. If $\vec{\lambda}$ is the vector of eigenvalues, then $\frac{100}{\sum{\vec{\lambda}}} \vec{\lambda}$ tells us what percentage of the variance is explained by each eigenvalue.
```{r}
D.perc <- D / sum(D) * 100
plot(
  D.perc,
  xaxt = "n",
  pch = 16,
  cex = 1.5,
  type = "b",
  main = " ",
  ylab = "%"
)
axis(side=1, at=c(1:n))
```

The first eigenvector captures about 33\% of variance, while the tenth eigenvector contributes almost nothing, implying that it is nearly a linear combination of other vectors. There is a steep drop-off after our sixth eigenvalue, so six eigenvectors might be a good choice for our regression.

## Dimension Reduction
\footnotesize
If we cumulatively sum this vector of percentage contributions. using only the first five eigenvectors, we capture 89\% of variance. Using the first six eigenvectors, we capture 96\% of the variance. After that, the contributions taper off.

```{r, fig.height=6}
plot(
  cumsum(D.perc),
  pch = 16,
  cex = 1.5,
  type = "b",
  main = "Cumulative Percent of Variance Explained by Eigenvalues",
  ylab = "Cumulative %",
  xaxt = "n",
  yaxt = "n"
)
axis(side=1, at=c(1:n))
axis(side=2, at=round(cumsum(D.perc))[c(1:6)], lwd.ticks=.5)
```

# Example 1

## IMDb Data
\small
Let's now use principal component analysis to forecast IMDB user reviews for movies.

\scriptsize
```{r, echo=TRUE}
data <- read.csv("IMDb movies.csv")
names(data)
```

\scriptsize
Here we have data from the online movie database service IMDb.. We are using this dataset because we can expect a high degree of collinearity between variables such as `metascore` and `reviews_from_critics`, though this is not always the case.

## Preparing our data
\small
First we need to select the proper variables for this type of analysis. Since principal component regression examines the eigenvalues of a matrix, we'll stick with numerical data types. Principal component regression can be done with categorical variables, but we're sticking with the basics here.

\tiny
```{r, echo=TRUE}
revised_data <- data[substr(data$budget, 1, 1) == "$", ]
revised_data$budget <-
  substr(revised_data$budget, 2, stop = 1000) %>% as.numeric
my_data <-
  revised_data[sapply(revised_data, class) %in% c("numeric", "integer")] %>%
  na.omit %>%
  .[c(6, 1:5, 7)] # Reorder our dataframe to put metascore at the front
summary(my_data)
```

## Preparing our data
\small
And now we normalize our data columns using the two-norm and center it so that $\mu=0$. 

\scriptsize
```{r, echo=TRUE}
my_data <- my_data %>%
  data.Normalization(type = "n12", normalization = "column")
```

\small
Our data is now scaled and centered, making it ready for principal component regression. Note that we will get differently scaled eigenvalues when we use a different scaling algorithm, but our final results will be the same.

## Principal Components
\tiny
```{r}
data.X <- my_data[2:7] %>% as.matrix
data.eig <- eigen(t(data.X) %*% data.X)
data.D <- data.eig$values
n <- length(data.D)
plot(
  data.D,
  xaxt = "n",
  pch = 16,
  cex = 1.5,
  type = "b",
  ylab = expression(lambda),
  xlab = ""
)
axis(side=1, at=c(1:n))
```

```{r}
data.D.perc <- data.D / sum(data.D) * 100
plot(
  cumsum(data.D.perc),
  pch = 16,
  cex = 1.5,
  type = "b",
  ylab = "Cumulative %",
  xaxt = "n",
  yaxt = "n",
  xlab = "Principal Component"
)
axis(side=1, at=c(1:n))
axis(side=2, at=round(cumsum(data.D.perc))[c(1:(n-1))], lwd.ticks=.5)
```

## Full PC Rank Regression
\footnotesize
Let's calculate a regression using all the principal components.

\tiny
```{r, echo=T}
data.V <- data.eig$vectors
data.Z <- data.X %*% data.V
data.Y <- my_data[1] %>% as.matrix # Normalized and scaled metascore
data.model1.z <- lm(data.Y ~ data.Z)
```

```{r}
summary(data.model1.z)
```

\footnotesize
In this model, all our principal components are significant, and our $R^2=0.5946$.

## Reduced Principal Components
\footnotesize
Now, let's compare this to our model if we use five principal components, accounting for about $96\%$ of our matrix's variance. We set our $V$ to just the first five eigenvectors (principal components), then repeat the above steps.

\tiny
```{r, echo=T}
data.V <- data.eig$vectors[,1:5]
```

```{r}
data.Z <- data.X %*% data.V
data.model2.z <- lm(data.Y ~ data.Z)
summary(data.model2.z)
```


\footnotesize
Again, all of our principal components are significant, and our $R^2=0.5889$. Although we've removed predictor variable that was considered significant, the two models have very similar $R^2$ values. 

## Further Reducing Principal Components
\footnotesize
Now, let's create a model using four principal components, accounting for $90\%$ of the variance in our data. We set our $V$ to just the first four eigenvectors, then repeat the above steps.

\tiny
```{r, echo=T}
data.V <- data.eig$vectors[,1:4]
```

```{r}
data.Z <- data.X %*% data.V
data.model3.z <- lm(data.Y ~ data.Z)
summary(data.model3.z)
```

\footnotesize
With just four principal components, we get $R^2=0.5882$, compared to $R^2=0.5889$ for five principal components and $R^2=0.5946$ for all six.


## Too Few Principal Components
\footnotesize
Now, let's create a model using three principal components, accounting for $83\%$ of the variance in our data. We set our $V$ to just the first three eigenvectors, then repeat the above steps.

\tiny
```{r, echo=T}
data.V <- data.eig$vectors[,1:3]
```

```{r}
data.Z <- data.X %*% data.V
data.model4.z <- lm(data.Y ~ data.Z)
summary(data.model4.z)
```

\footnotesize
With three principal components, we get $R^2=0.5423$.


## Compare to Ordinary Least-Squares Regression
\footnotesize
Let's compare our models to an ordinary least-squares regression model.

\tiny
```{r, echo=T}
data.ols1 <- lm(reviews_from_users ~ ., data=my_data)
summary(data.ols1)
```

\footnotesize
With only six predictor columns, it is no surprise that all of them are significant. Notice that our $R^2$ of $0.5946$ is the same value as our full PC rank model.

## Reducing Predictors of our OLS
\footnotesize
Say we want to reduce the number of predictor variables in our regression. Since our data is scaled, we can see the contribution of our points by their $\beta$ value.

```{r}
temp.frame <- round(data.ols1$coefficients[order(abs(data.ols1$coefficients))], 4) %>%
  as.data.frame
colnames(temp.frame) <- expression(beta)
kable(temp.frame)
```

\footnotesize
Now let's compare how dimension reduction affects $R^2$ for ordinary least-squares compared to principal component regression.

## Comparing $R^2$ for OLS and PCR

```{r}
data.V <- data.eig$vectors[,1:2]
data.Z <- data.X %*% data.V
data.model5.z <- lm(data.Y ~ data.Z)

data.V <- data.eig$vectors[,1]
data.Z <- data.X %*% data.V
data.model6.z <- lm(data.Y ~ data.Z)

data.ols2 <- lm(reviews_from_users ~ ., data=my_data[-c(6)])
data.ols3 <- lm(reviews_from_users ~ ., data=my_data[-c(6, 3)])
data.ols4 <- lm(reviews_from_users ~ ., data=my_data[-c(6, 3, 2)])
data.ols5 <- lm(reviews_from_users ~ ., data=my_data[-c(6, 3, 2, 5)])
data.ols6 <- lm(reviews_from_users ~ ., data=my_data[-c(6, 3, 2, 5, 7)])


ols.rs <- c(
  summary(data.ols6)$r.squared,
  summary(data.ols5)$r.squared,
  summary(data.ols4)$r.squared,
  summary(data.ols3)$r.squared,
  summary(data.ols2)$r.squared,
  summary(data.ols1)$r.squared
)

pcr.rs <- c(
  summary(data.model6.z)$r.squared,
  summary(data.model5.z)$r.squared,
  summary(data.model4.z)$r.squared,
  summary(data.model3.z)$r.squared,
  summary(data.model2.z)$r.squared,
  summary(data.model1.z)$r.squared
)

plot(
  pcr.rs,
  type = "b",
  pch = 16,
  cex = 1,
  col = "darkblue",
  ylab = "R-squared",
  xlab = "Number of predictors",
  main = "R-squared for OLS vs PCR",
  lwd = 1.5
)
points(
  ols.rs,
  type = "b",
  pch = 16,
  col = "darkorange",
  lwd = 1.5
)
legend(
  4.7,
  .475,
  legend = c("PCR", "OLS"),
  col = c("darkblue", "darkorange"),
  lty = 1,
  cex = 1,
  lwd = 2
)
```

\small
What may be surprising here is that using principal component regression is not helpful in this example! This is because our principal components are just calculated using our predictor variables, and so while our first principal component might maximize variance in our data, it does not necessarily maximize impact on our response variable.

## Why Not PCR
\small
In our example, we assumed that `metascore` and `reviews_from_critics` faced potential collinearity. This turned out not to be the case, however.

\scriptsize
```{r, echo=TRUE}
vif(data.ols1)
```

\small
This indicates low levels of collinearity. We also only had only six predictor columns, and PCR is most effective when there are a large number of columns compared to rows. It can especially be employed when we have more columns than rows!

# Example 2

## compare to MLR

\small

I will use a small example to show how PCR is better than MLR in some cases
```{r}
summary(mtcars)
```
---
\small
split dataset into training set and test set
```{r}
smp_size <- floor(0.80 * nrow(mtcars))

## set the seed 
set.seed(123)
train_ind <- sample(seq_len(nrow(mtcars)), size = smp_size)

train_set <- mtcars[train_ind, ]
test_set <- mtcars[-train_ind, ]

```
---
\small
This time let's analyze how mpg can be affected by all the other factors.


let's try multiple linear regression first 
---
```{r}
mlr<-lm(mpg~., data = train_set)
summary(mlr)
```
---
```{r}
mlr_pred<-predict(mlr,test_set)
mean((mlr_pred-test_set$mpg)^2)
```
\small

now let's try to do pcr for this dataset
```{r}
pcr<-pcr(mpg~., data = train_set, scale = TRUE, validation = "CV")
summary(pcr)
```
```{r}
pcr_pred<-predict(pcr, test_set, ncomp = 7)
mean((pcr_pred-test_set$mpg)^2)
```

From the above we can see that our mse is less for PCR than for MLR

# compare to MLR

\small

I will use a small example to show how PCR is better than MLR in some cases

---
\tiny
```{r,echo=TRUE}
summary(mtcars)
```

---
\small
split dataset into training set and test set

```{r,echo=TRUE}
smp_size <- floor(0.80 * nrow(mtcars))

## set the seed 
set.seed(123)
train_ind <- sample(seq_len(nrow(mtcars)), size = smp_size)

train_set <- mtcars[train_ind, ]
test_set <- mtcars[-train_ind, ]

```

---
\small
This time let's analyze how mpg can be affected by all the other factors.


let's try multiple linear regression first 
---
```{r,echo=TRUE}
mlr<-lm(mpg~., data = train_set)
summary(mlr)
```


--- 
```{r,echo=TRUE}
mlr_pred<-predict(mlr,test_set)
mean((mlr_pred-test_set$mpg)^2)
```

---
now let's try to do pcr for this dataset

```{r,echo=TRUE}
library(pls)
pcr<-pcr(mpg~., data = train_set, scale = TRUE, validation = "CV")
summary(pcr)
```
```{r}
pcr_pred<-predict(pcr, test_set, ncomp = 7)
mean((pcr_pred-test_set$mpg)^2)
```
